{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# AR Likelihood for CBC Inference — GWOSC Demo (Clean + Injection)\n",
    "\n",
    "This notebook demonstrates a **time-domain autoregressive (AR) likelihood** for compact binary coalescence (CBC) inference using **real LIGO data** from **GWOSC**. It:\n",
    "\n",
    "- downloads a short strain segment with **GWPy**\n",
    "- pre-processes (high-pass, optional resample)\n",
    "- estimates **AR(p)** noise model on off-source data (Yule–Walker)\n",
    "- builds an **AR-whitened Gaussian likelihood**\n",
    "- injects a CBC (via **PyCBC**), then compares log-likelihoods for:\n",
    "  - **h = 0** (noise-only)\n",
    "  - **h = template** (correct signal model)\n",
    "\n",
    "> Requirements: `gwpy`, `pycbc`, `numpy`, `scipy`, `matplotlib`  \n",
    "> Run the install cell below if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on a fresh environment, uncomment to install dependencies:\n",
    "# %pip install -q gwpy pycbc numpy scipy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gwpy.timeseries import TimeSeries\n",
    "from scipy.signal import welch\n",
    "from scipy.signal.windows import tukey\n",
    "from scipy.linalg import solve_toeplitz\n",
    "from scipy.special import erfinv\n",
    "\n",
    "# PyCBC for CBC waveform generation\n",
    "from pycbc.waveform import get_td_waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Configuration ----------------\n",
    "# Data span near GW150914 (public, convenient)\n",
    "EVENT_GPS   = 1126259462.4\n",
    "SPAN_SEC    = 32.0\n",
    "IFO         = \"L1\"\n",
    "\n",
    "# Preprocessing\n",
    "HIGHPASS_HZ = 20.0\n",
    "RESAMPLE_HZ = 2048.0       # set to None to keep native rate\n",
    "\n",
    "# AR model\n",
    "AR_ORDER    = 80           # try 40..200; larger -> smoother PSD model\n",
    "\n",
    "# Segments used to estimate AR from off-source (in seconds relative to segment start)\n",
    "# We'll estimate AR on two 6 s regions away from center to avoid any real signal\n",
    "AR_TRAIN_WINDOWS = [(2.0, 8.0), (SPAN_SEC-8.0, SPAN_SEC-2.0)]\n",
    "\n",
    "# Injection (PyCBC / IMRPhenomD, non-spinning)\n",
    "DO_INJECTION = True\n",
    "M1_Msun      = 30.0\n",
    "M2_Msun      = 30.0\n",
    "DIST_Mpc     = 400.0        # lower -> louder\n",
    "F_LOWER      = 30.0\n",
    "APPROXIMANT  = \"IMRPhenomD\"\n",
    "INJ_INCL     = 0.0          # face-on\n",
    "INJ_PHI      = 0.0\n",
    "INJ_GPS      = EVENT_GPS    # coalescence time at center\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Fetch GWOSC data ----------------\n",
    "start = EVENT_GPS - SPAN_SEC/2\n",
    "stop  = EVENT_GPS + SPAN_SEC/2\n",
    "print(f\"Fetching {IFO} strain from {start:.1f} to {stop:.1f} GPS ...\")\n",
    "data = TimeSeries.fetch_open_data(IFO, start, stop)  # requires internet\n",
    "data = data.highpass(HIGHPASS_HZ)\n",
    "if RESAMPLE_HZ is not None:\n",
    "    data = data.resample(RESAMPLE_HZ)\n",
    "\n",
    "t  = data.times.value\n",
    "x0 = data.value.astype(float)\n",
    "fs = float(data.sample_rate.value)\n",
    "n  = len(x0)\n",
    "\n",
    "print(f\"Sample rate: {fs:.1f} Hz, samples: {n}, duration: {n/fs:.1f} s\")\n",
    "plt.figure(figsize=(11,3.5))\n",
    "plt.plot(t - t[0], x0, lw=0.6)\n",
    "plt.xlabel(\"Time since segment start [s]\")\n",
    "plt.ylabel(\"Strain (hp)\")\n",
    "plt.title(f\"{IFO} strain — {SPAN_SEC:.0f}s, fs={fs:.0f} Hz\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- AR estimation (Yule–Walker) ----------------\n",
    "def autocorr_biased(x, maxlag):\n",
    "    x = np.asarray(x)\n",
    "    n = len(x)\n",
    "    ac = np.correlate(x, x, mode='full')\n",
    "    ac = ac[n-1 - maxlag : n + maxlag]  # not strictly needed; keep full if desired\n",
    "    ac_full = np.correlate(x, x, mode='full')[n-1: n-1+maxlag+1]\n",
    "    return ac_full / n\n",
    "\n",
    "def estimate_ar_yw(x, order):\n",
    "    \"\"\"Estimate AR(p) via Yule–Walker.\n",
    "    Returns phi (length p) and innovation variance sigma2.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x) - np.mean(x)\n",
    "    r = autocorr_biased(x, order)  # r[0..order]\n",
    "    R = solve_toeplitz((r[:-1], r[:-1]), r[1:])  # Toeplitz from r[0..p-1], RHS r[1..p]\n",
    "    phi = R\n",
    "    sigma2 = r[0] - np.dot(phi, r[1:])\n",
    "    return phi, float(max(sigma2, np.finfo(float).tiny))\n",
    "\n",
    "def apply_ar_whitener(x, phi):\n",
    "    p = len(phi)\n",
    "    x = np.asarray(x)\n",
    "    y = np.zeros_like(x)\n",
    "    for t_idx in range(p, len(x)):\n",
    "        y[t_idx] = x[t_idx] - np.dot(phi, x[t_idx-p:t_idx][::-1])\n",
    "    return y\n",
    "\n",
    "def loglike_ar(d, h, phi, sigma2, drop=0):\n",
    "    \"\"\"AR Gaussian log-likelihood (white innovations).\n",
    "    drop: number of initial samples to drop (e.g., p) to avoid filter transients.\n",
    "    \"\"\"\n",
    "    wd = apply_ar_whitener(d, phi)\n",
    "    wh = apply_ar_whitener(h, phi)\n",
    "    r = wd - wh\n",
    "    if drop > 0:\n",
    "        r = r[drop:]\n",
    "    N = len(r)\n",
    "    return -0.5 * (np.dot(r, r) / sigma2 + N * np.log(2*np.pi*sigma2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Build AR training vector ----------------\n",
    "def time_to_index(t0, t1, t_array):\n",
    "    # returns slice indices covering [t0, t1)\n",
    "    t_rel = t_array - t_array[0]\n",
    "    a = np.searchsorted(t_rel, t0, side='left')\n",
    "    b = np.searchsorted(t_rel, t1, side='left')\n",
    "    return slice(a, b)\n",
    "\n",
    "train_chunks = []\n",
    "for (a, b) in AR_TRAIN_WINDOWS:\n",
    "    sl = time_to_index(a, b, t)\n",
    "    if sl.stop - sl.start > AR_ORDER + 10:\n",
    "        train_chunks.append(x0[sl])\n",
    "\n",
    "if not train_chunks:\n",
    "    raise RuntimeError(\"No valid AR training chunks; adjust AR_TRAIN_WINDOWS.\")\n",
    "\n",
    "x_train = np.concatenate(train_chunks)\n",
    "phi, sigma2 = estimate_ar_yw(x_train, AR_ORDER)\n",
    "print(f\"Estimated AR(p={AR_ORDER}) sigma^2 (innovation): {sigma2:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Diagnostics: whitening & PSD ----------------\n",
    "xw = apply_ar_whitener(x0, phi)\n",
    "\n",
    "plt.figure(figsize=(11,3.5))\n",
    "plt.plot(t - t[0], xw, lw=0.5)\n",
    "plt.xlabel(\"Time since segment start [s]\")\n",
    "plt.ylabel(\"AR-whitened strain\")\n",
    "plt.title(\"AR-whitened time series (should look ~white)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Welch PSDs\n",
    "def plot_psd(sig, fs, label, color=None):\n",
    "    nper = int(2.0 * fs)  # ~2 s windows\n",
    "    nover = int(0.5 * nper)\n",
    "    f, Pxx = welch(sig, fs=fs, nperseg=nper, noverlap=nover, detrend='constant')\n",
    "    plt.semilogy(f, Pxx, label=label)\n",
    "\n",
    "plt.figure(figsize=(11,3.8))\n",
    "plot_psd(x0, fs, \"raw (hp)\")\n",
    "plot_psd(xw[AR_ORDER:], fs, \"AR-whitened (drop p)\")\n",
    "plt.xlabel(\"Frequency [Hz]\"); plt.ylabel(\"PSD [strain^2/Hz]\"); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Generate CBC injection (time-domain) ----------------\n",
    "def make_td_injection(fs, n, start_gps, inj_gps, m1, m2, dist_mpc, f_lower, incl, phi0, approximant):\n",
    "    dt = 1.0 / fs\n",
    "    hp, hc = get_td_waveform(approximant=approximant,\n",
    "                             mass1=m1, mass2=m2, spin1z=0, spin2z=0,\n",
    "                             f_lower=f_lower, delta_t=dt, distance=dist_mpc,\n",
    "                             inclination=incl, coa_phase=phi0)\n",
    "    h = hp.numpy()  # plus polarization\n",
    "    inj_index = int(round((inj_gps - start_gps) * fs))\n",
    "    L = len(h)\n",
    "    x_model = np.zeros(n, dtype=float)\n",
    "    a = inj_index - (L - 1)\n",
    "    b = inj_index + 1\n",
    "    aa = max(a, 0); bb = min(b, n)\n",
    "    ha = aa - a; hb = ha + (bb - aa)\n",
    "    if bb > aa and hb > ha:\n",
    "        x_model[aa:bb] += h[ha:hb]\n",
    "    return x_model\n",
    "\n",
    "template_td = make_td_injection(fs, n, t[0], INJ_GPS, M1_Msun, M2_Msun, DIST_Mpc,\n",
    "                                F_LOWER, INJ_INCL, INJ_PHI, APPROXIMANT)\n",
    "\n",
    "x_clean = x0.copy()\n",
    "x_inj   = x0 + template_td if DO_INJECTION else x0\n",
    "\n",
    "plt.figure(figsize=(11,3.5))\n",
    "plt.plot(t - t[0], x_inj, lw=0.6, label=\"data + injection\" if DO_INJECTION else \"data\")\n",
    "if DO_INJECTION:\n",
    "    plt.plot(t - t[0], template_td, lw=0.6, alpha=0.8, label=\"injected template\")\n",
    "plt.xlabel(\"Time since segment start [s]\"); plt.ylabel(\"Strain\")\n",
    "ttl = f\"Time series (fs={fs:.0f} Hz)\"\n",
    "if DO_INJECTION: ttl += f\"  — inj {M1_Msun:.0f}+{M2_Msun:.0f} Msun @ {DIST_Mpc:.0f} Mpc\"\n",
    "plt.title(ttl); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- AR likelihood comparison ----------------\n",
    "drop = AR_ORDER  # drop first p samples to avoid filter transient in likelihood\n",
    "\n",
    "def ll_summary(x_data, name):\n",
    "    ll_h0 = loglike_ar(x_data, np.zeros_like(x_data), phi, sigma2, drop=drop)\n",
    "    ll_ht = loglike_ar(x_data, template_td,               phi, sigma2, drop=drop)\n",
    "    print(f\"{name:12s}  logL(h=0): {ll_h0: .3f}    logL(h=template): {ll_ht: .3f}    Δ: {ll_ht-ll_h0: .3f}\")\n",
    "    return ll_h0, ll_ht\n",
    "\n",
    "print(\"AR-likelihood (white innovations) results:\")\n",
    "ll_clean = ll_summary(x_clean, \"clean\")\n",
    "ll_inj   = ll_summary(x_inj,   \"injected\" if DO_INJECTION else \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- QQ plot of AR-whitened residuals ----------------\n",
    "def qq_plot(z, title):\n",
    "    z = z[np.isfinite(z)]\n",
    "    z.sort()\n",
    "    p = (np.arange(len(z)) + 0.5)/len(z)\n",
    "    qn = np.sqrt(2) * erfinv(2*p - 1)\n",
    "    plt.figure(figsize=(4.6,4.6))\n",
    "    plt.plot(qn, z, \".\", ms=2)\n",
    "    lim = np.percentile(np.abs(z), 99.5)\n",
    "    plt.plot([-lim, lim], [-lim, lim], lw=1)\n",
    "    plt.xlabel(\"Normal quantiles\")\n",
    "    plt.ylabel(\"Residual quantiles\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# residuals for injected case, under h=0 and h=template\n",
    "wd = apply_ar_whitener(x_inj, phi)[drop:]\n",
    "wh0 = apply_ar_whitener(np.zeros_like(x_inj), phi)[drop:]\n",
    "wht = apply_ar_whitener(template_td, phi)[drop:]\n",
    "\n",
    "r_h0 = wd - wh0\n",
    "r_ht = wd - wht\n",
    "\n",
    "qq_plot(r_h0/np.sqrt(sigma2), \"QQ: whitened residuals (h=0 model)\")\n",
    "qq_plot(r_ht/np.sqrt(sigma2), \"QQ: whitened residuals (template model)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Next Steps\n",
    "\n",
    "- **Model order (`AR_ORDER`)**: try 40–200. Higher order models finer spectral detail; beware overfit if the off-source is too short.\n",
    "- **Training windows**: change `AR_TRAIN_WINDOWS` to avoid any on-source signal; use longer, quieter data to learn noise better.\n",
    "- **Injection**: modify masses/distance, or set `DO_INJECTION=False` to analyze clean data.\n",
    "- **Extensions**:\n",
    "  - Student-t AR innovations (robust to glitches)\n",
    "  - Time-varying AR (Kalman) for non-stationary drift\n",
    "  - Multi-detector combination with per-site AR models\n",
    "  - Wrap this likelihood inside an MCMC/Nested sampler to recover posteriors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
